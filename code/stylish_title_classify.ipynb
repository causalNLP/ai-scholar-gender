{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17012,"status":"ok","timestamp":1655662575400,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"t2AVIwPdks6r","outputId":"7fa1b85e-0afe-4def-eeae-c7482f621206"},"outputs":[],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab1/ai_scholar/'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2300,"status":"ok","timestamp":1655662581580,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"qPftxRbekjPp"},"outputs":[],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","import csv\n","import numpy as np\n","import nltk\n","import sys\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from pathlib import Path\n","import pickle\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":805,"status":"ok","timestamp":1655662582381,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"awIgYjp8dQqT","outputId":"8b344111-f432-40b7-b381-509bb84145aa"},"outputs":[],"source":["nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"RvE-XnGgkjPs"},"source":["Other things to try:\n","- dependency parsing to check questions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655662582382,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"A1v3NzO8kjPt"},"outputs":[],"source":["def set_seed(seed=0):\n","    np.random.seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1655662583545,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"U6EMNqiEkjPt"},"outputs":[],"source":["def is_good(title):\n","    \"\"\"Check whether a title is stylish.\"\"\"\n","    title = title.strip()\n","    # semicolon\n","    title_split = title.split(':', 1)\n","    flag_semicolon = False\n","    if len(title_split) > 1:\n","        # if len(word_tokenize(title_split[0])) == 1:\n","        title = title_split[0].strip() +' '+ title_split[1].strip()\n","        flag_semicolon = True\n","    else:\n","        title = title_split[0]\n","\n","    # deal with specific words\n","    title = title.lower()\n","    if 'www' in title or 'journal' in title or 'volume' in title or 'conference' in title or 'copyright' in title or 'vol' in title or 'ieee' in title:\n","        return False\n","\n","    # other punctuations\n","    cnt_ques, cnt_quot = 0, 0\n","    for idx, char in enumerate(title):\n","        if char == '!':\n","            return True\n","        if idx != len(title) - 1 and char == '.' and (title[idx+1] == ' ' and title[idx-1] != '.'):\n","            try:\n","                if word_tokenize(title[:idx])[-1][0].islower() and word_tokenize(title[:idx])[-2][0].islower():\n","                    # print('yes4')\n","                    return True\n","            except IndexError:\n","                pass\n","        if char == \"'\":\n","            flag1 = False\n","            flag2 = False\n","            if idx == 0 or (idx > 0 and title[idx-1] == ' '):\n","                flag1 = True\n","            if idx == len(title) - 1 or (idx < len(title) - 1 and title[idx+1] == ' '):\n","                flag2 = True\n","            if flag1 or flag2:\n","                cnt_quot += 1\n","        if char == '?':\n","            cnt_ques += 1\n","    if cnt_ques == 1:\n","        return True\n","    if cnt_quot == 2:\n","        return True\n","    # detect pronouns and numeric\n","    # https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n","    token_list = word_tokenize(title)\n","    tagged_list = nltk.pos_tag(token_list)\n","    # print(tagged_list)\n","    # flag_numeric = False\n","    for tag_idx, tag_tuple in enumerate(tagged_list):\n","        if flag_semicolon and tag_idx == 0 and (tag_tuple[1] == 'TO' or tag_tuple[1][:2] == 'VB') and tag_tuple[0] != 'search':\n","            # print('yes1')\n","            return True\n","        elif tag_tuple[1] == 'PRP' or tag_tuple[1] == 'PRP$':\n","            tmp_pron = tag_tuple[0].lower()\n","            if tmp_pron != 's' and tmp_pron != 'it' and tmp_pron != 'its' and tmp_pron != 'i' and tmp_pron != 'their':\n","                # print('yes2')\n","                return True\n","        elif tag_idx == 0 and tag_tuple[1] == 'CD' and len(tag_tuple[0]) != 4:\n","            if tag_idx < len(tagged_list)-1 and (tag_tuple[0].isnumeric() or not tag_tuple[0][0].isnumeric()):\n","                tmp_str = ' '.join(token_list[tag_idx:tag_idx+4])\n","                if not ('=' in tmp_str or '√' in tmp_str or 'TeV' in tmp_str or 'GeV' in tmp_str or '*' in tmp_str or '/' in tmp_str or '\\\\' in tmp_str):\n","                    # print('yes3')\n","                    tmp_str2 = tagged_list[tag_idx+1][1][:2]\n","                    if tmp_str2 == 'JJ' or tmp_str2 == 'NN' or tmp_str2 == 'RB':\n","                        return True\n","                # flag_numeric = True\n","        elif tag_tuple[1] == ',':\n","            try:\n","                tmp_word = tagged_list[tag_idx+2][1]\n","                tmp_tag = tagged_list[tag_idx+1][1][:2]\n","                if tmp_word == 'CC' and (tmp_tag == 'NN' or tmp_tag == 'VB'):\n","                    return True\n","            except IndexError:\n","                pass\n","            try:\n","                tmp_word = tagged_list[tag_idx+3][1]\n","                tmp_tag = tagged_list[tag_idx+2][1][:2]\n","                if tmp_word == 'CC' and (tmp_tag == 'NN' or tmp_tag == 'VB'):\n","                    return True\n","            except IndexError:\n","                pass\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1652657539578 ,"user_tz":240},"id":"RNREuM9-5wh_","outputId":"ed0a317d-019e-496c-8edc-4a9937200f22"},"outputs":[],"source":["print(nltk.pos_tag(word_tokenize(\"Search for top squark pair production using dilepton final states in [... formula...] collision data collected at [... formula...]\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1652656410619 ,"user_tz":240},"id":"g6i-T-Nc_fGY","outputId":"ac9a4594-66b2-485e-b6ad-b8fe8111b342"},"outputs":[],"source":["token_list = 'sgew'\n","tag_idx = 2\n","try:\n","  token_list[tag_idx+2]\n","except IndexError:\n","  print('yes')\n","print(1+2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1652661450144 ,"user_tz":240},"id":"xngkz5D8kjPu","outputId":"80c69b41-6233-4d06-99ed-dcb671a58a22"},"outputs":[],"source":["# a quick check of the functionality\n","print(is_good(\"Search for direct production of charginos, neutralinos and sleptons in final states with two leptons and missing transverse momentum in pp collisions at = 8TeV …\"))"]},{"cell_type":"markdown","metadata":{"id":"wnVocrqm9wRu"},"source":["# look at scholars that tend to write stylish titles, use 78k_scholar_new.npy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlTZtN2i9v6t"},"outputs":[],"source":["scholar_all = np.load(GOOGLE_DRIVE_PATH+'gegs_scholars.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BT2QE7YA-4ct"},"outputs":[],"source":["path = GOOGLE_DRIVE_PATH+\"author_gender.csv\" #author_gender\n","import pandas as pd\n","df = pd.read_csv(path, names=('name', 'url', 'gender'))\n","df.insert(0, 'index', df.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhGK26S1-ibc"},"outputs":[],"source":["# scholar_all[0]['papers'] # a list of papers\n","from collections import defaultdict\n","sty_dict = defaultdict(float)\n","for scholar in scholar_all:\n","  for paper in scholar['papers']:\n","    sty_dict[scholar['url']] += int(is_good(paper[1]))\n","  # sty_dict[scholar['url']] /= len(scholar['papers'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qesP7t5tU0yZ"},"outputs":[],"source":["import pickle\n","sorted_sty_dict = {k: v for k, v in sorted(sty_dict.items(), key=lambda item: item[1], reverse=True)}\n","with open(GOOGLE_DRIVE_PATH+'out/sorted_sty_dict1.pkl', 'wb') as file:\n","  pickle.dump(sorted_sty_dict, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":579,"status":"ok","timestamp":1655664269941,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"IZyNdUOhnbNw"},"outputs":[],"source":["with open(GOOGLE_DRIVE_PATH+'out/sorted_sty_dict1.pkl', 'rb') as file:\n","  sorted_sty_dict = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PubmHFWsVwNm"},"outputs":[],"source":["sorted_abs_sty_dict = {k: v for k, v in sorted(sty_dict.items(), key=lambda item: item[1], reverse=True)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4yNj4ISV-L0"},"outputs":[],"source":["with open(GOOGLE_DRIVE_PATH+'out/sorted_abs_sty_dict1.pkl', 'wb') as file:\n","  pickle.dump(sorted_abs_sty_dict, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":722,"status":"ok","timestamp":1655663437727,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"aq4fub1FZUm1"},"outputs":[],"source":["with open(GOOGLE_DRIVE_PATH+'out/sorted_abs_sty_dict1.pkl', 'rb') as file:\n","  sorted_abs_sty_dict = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":72664,"status":"ok","timestamp":1655663611039,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"nvFY9JmiPFYf"},"outputs":[],"source":["scholar_all = np.load(GOOGLE_DRIVE_PATH+'general/gs_scholars_all_upd.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1655663613562,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"hLjUpNutPUmB"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame.from_records(scholar_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17070,"status":"ok","timestamp":1655665425316,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"pQ6rELMQZgbA","outputId":"54c967fb-7e47-4e36-b2f7-196e043c0c5a"},"outputs":[],"source":["# tab:stattopsty\n","from collections import defaultdict\n","cnt = 0\n","gender_dict = defaultdict(int)\n","for key, value in sorted_abs_sty_dict.items():\n","  if cnt >= 300:\n","    break\n","  gender_dict[df.loc[df['url'] == key, 'gender'].item()] += 1\n","  # print(df.loc[df['url'] == key, 'gender'].item())\n","  if df.loc[df['url'] == key, 'gender'].item() == 'F':\n","    print(key)\n","  cnt += 1\n","print(gender_dict) # Priyanka Sharma"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":115,"status":"ok","timestamp":1655665455245,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"1qh2ekNsTtkI"},"outputs":[],"source":["tmp = 'https://scholar.google.com/citations?hl=en&user=Joq4sP8AAAAJ'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1655665456629,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"rekxbW2nRV20","outputId":"a200ead2-4dab-4480-8562-7eeb41c88f9b"},"outputs":[],"source":["sorted_abs_sty_dict[tmp]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1655665457639,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"nOQiLAmnR-Xt","outputId":"94b76477-283a-4d76-f31a-2ff622621861"},"outputs":[],"source":["sorted_sty_dict[tmp]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1435,"status":"ok","timestamp":1655665459748,"user":{"displayName":"Jerry Liu","userId":"13954054572437395358"},"user_tz":240},"id":"h0yjxXRFFOFP","outputId":"b755cd72-7c5a-4db6-94f7-81076eeb17df"},"outputs":[],"source":["cnt = 0\n","for scholar in scholar_all:\n","  if scholar['url'] == tmp:\n","    for paper in scholar['papers']:\n","      if is_good(paper[1]):\n","        print(paper[1])\n","        cnt += 1\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1652675477315 ,"user_tz":240},"id":"ufxfdWH4Tz8X","outputId":"e1cec0af-db0a-4466-8fa6-dfe402248cad"},"outputs":[],"source":["print(sorted_sty_dict['https://scholar.google.com/citations?hl=en&user=3FWe5OQAAAAJ'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1652673086301 ,"user_tz":240},"id":"ugGWIjp5-a9W","outputId":"f5f190d0-4a4e-4670-f0af-61ce47bd6cd9"},"outputs":[],"source":["# print statistics\n","cnt = 0\n","for key, value in sorted_abs_sty_dict.items():\n","  if cnt >= 10:\n","    break\n","  print(key, value)\n","  cnt += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQQBuTI9eGBu"},"outputs":[],"source":["# dataframe to latex?\n"]},{"cell_type":"markdown","metadata":{"id":"LDJ0EM6y96Pu"},"source":["# use original 100k_updated_with_full_name.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_FW-S6CkjPv"},"outputs":[],"source":["def gen_test(num_samples, seed):\n","    \"\"\"Generate random samples.\"\"\"\n","    set_seed(seed)\n","    sample_idxes = []\n","    csv.field_size_limit(sys.maxsize)\n","    with open(GOOGLE_DRIVE_PATH+'data/100k_updated_with_full_name.csv', 'r') as csvfile:\n","        reader_list = np.array([row['title'] for row in csv.DictReader(csvfile, delimiter=',', quotechar='\"')])\n","        # print(reader_list)\n","        sample_idxes = np.random.choice(len(reader_list), size=num_samples, replace=False)\n","        # print(sample_idxes)\n","        return reader_list[sample_idxes], sample_idxes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNDdem4ckjPv"},"outputs":[],"source":["def out_test(seed):\n","    \"\"\"Test and see random sample titles from 78k dataset.\"\"\"\n","    reader_list, _ = gen_test(100, seed)\n","    # print(reader_list)\n","    predicts = []\n","    for title in reader_list:\n","        predicts.append(int(is_good(title)))\n","    with open(GOOGLE_DRIVE_PATH+f'out/new_test{seed}.csv', 'w') as outfile:\n","        outfile.write('title,class\\n')\n","        outfile.write('\\n'.join([f'\"{reader_list[idx]}\",{str(pred)}' for idx, pred in enumerate(predicts)]))\n","out_test(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4h9XoHPRfLj8"},"outputs":[],"source":["# cnn clickbait detector\n","def cnn_out_test(seed):\n","    reader_list, sample_idxes = gen_test(100, seed)\n","    with open(GOOGLE_DRIVE_PATH+f'data/cnn_clickbait_output.txt', 'r') as infile:\n","        total_predicts = [float(line.strip()) for line in infile.readlines()]\n","    predicts = []\n","    for idx in sample_idxes:\n","        if total_predicts[idx] >= 50:\n","            predicts.append(1)\n","        else:\n","            predicts.append(0)\n","    with open(GOOGLE_DRIVE_PATH+f'out/cnn_clickbait/new_test{seed}.csv', 'w') as outfile:\n","        outfile.write('title,class\\n')\n","        outfile.write('\\n'.join([f'\"{reader_list[idx]}\",{str(pred)}' for idx, pred in enumerate(predicts)]))\n","cnn_out_test(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1652236922617 ,"user_tz":240},"id":"tiWjSE0kkjPw","outputId":"2ce4fcc0-f875-48e4-f027-861c46293b8c"},"outputs":[],"source":["# after manual labelling, check F1 score:\n","def check_score(seed):\n","    predicts = []\n","    with open(GOOGLE_DRIVE_PATH+f'out/new_test{seed}.csv', 'r') as csvfile: # cnn or is_good\n","        reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n","        for row in reader:\n","            # print(row['title'])\n","            predicts.append(int(row['class']))\n","    if seed == 0:\n","        labels = [0,0,0,0,0,0,1,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,1,0,0,0,1,0,0,0,0,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,0,1,0,1,0,0,\n","                  0,0,1,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,0,\n","                  1,0,0,0,0,0,0,0,0,0]\n","    elif seed == 43:\n","        labels = [0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,1,0,0,0,1,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,1,0,0,0,0,0,0,0,\n","                  1,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,1,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  1,1,0,1,1,0,0,0,0,0]\n","    elif seed == 1:\n","        labels = [0,1,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,1,0,0,\n","                  0,0,0,0,1,0,0,1,1,0,\n","                  0,1,0,0,0,0,0,0,0,0,\n","                  0,1,0,0,0,0,0,0,0,1,\n","                  0,0,1,0,0,0,0,0,0,0,\n","                  0,0,0,0,1,1,0,0,0,0,\n","                  0,0,0,0,1,0,0,1,0,0,\n","                  1,0,1,0,0,0,0,0,0,0,\n","                  0,0,0,0,1,0,0,0,0,0]\n","    elif seed == 2:\n","        labels = [0,0,0,0,0,0,0,0,0,0,\n","                  0,0,1,0,0,0,0,0,1,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,1,0,0,0,0,0,1,0,\n","                  0,0,0,0,0,0,1,0,0,0,\n","                  0,1,0,0,0,0,1,0,1,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,1,0,0,0,0,0,1,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,1,0,0,1,0,1,0,0]\n","    elif seed == 3:\n","        labels = [0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,1,0,1,0,0,0,0,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  0,1,0,1,0,0,0,1,0,1,\n","                  0,0,1,0,0,0,0,1,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,1,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0]\n","    elif seed == 4:\n","        labels = [0,0,1,0,0,0,0,0,0,0,\n","                  0,1,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,1,1,\n","                  0,0,0,0,0,0,1,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,1,\n","                  0,0,0,0,0,0,0,1,0,1,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0]\n","    elif seed == 5:\n","        labels = [0,0,1,1,0,0,0,0,0,0,\n","                  1,0,0,0,0,0,0,1,0,0,\n","                  1,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,1,0,0,\n","                  1,0,0,0,1,1,0,0,0,0,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  1,0,0,0,0,0,0,0,0,1,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  1,0,0,1,0,0,0,0,1,0,\n","                  0,0,1,0,1,0,0,0,0,0,]\n","    elif seed == 6:\n","        labels = [0,0,0,0,0,0,1,1,0,0,\n","                  0,0,0,0,0,0,0,0,0,1,\n","                  0,0,0,0,0,0,1,0,0,0,\n","                  0,0,0,1,0,0,0,0,1,0,\n","                  0,0,1,0,0,0,0,0,0,0,\n","                  1,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,1,0,1,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,1,0,1,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,1,0]\n","    elif seed == 7:\n","        labels = [0,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,1,1,0,\n","                  0,1,0,1,0,0,0,0,0,0,\n","                  1,0,0,0,0,0,0,0,0,1,\n","                  0,1,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,1,\n","                  0,0,0,1,0,0,0,0,0,0,\n","                  0,0,0,0,1,0,0,0,0,0,\n","                  0,0,0,0,0,0,0,1,0,0]\n","    elif seed == 8:\n","        labels = [0,0,1,0,0,0,1,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,0,\n","                  0,1,0,0,0,0,0,1,0,0,\n","                  0,0,0,0,0,0,0,1,0,0,\n","                  1,0,0,0,0,0,1,0,1,0,\n","                  0,0,0,1,0,1,0,0,0,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  1,0,0,0,0,0,0,1,1,0,\n","                  0,0,0,0,0,0,0,0,0,0,\n","                  0,0,0,0,0,1,0,0,0,0]\n","    return labels, predicts\n","labels, predicts = [], []\n","for i in (43, 0, 1, 2, 3, 4, 5, 6, 7, 8):\n","    labels = labels + check_score(i)[0]\n","    predicts = predicts + check_score(i)[1]\n","print('F1 Score: %.3f' % f1_score(labels, predicts))\n","print('Precision Score: %.3f' % precision_score(labels, predicts))\n","print('Recall Score: %.3f' % recall_score(labels, predicts))"]},{"cell_type":"markdown","metadata":{"id":"bfwR9qEAbFBm"},"source":["the CNN model clickbait detector with bad performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTMKOt9skjPy","outputId":"f961d544-9f13-4125-e445-57fd6166d196"},"outputs":[],"source":["def out_manual_test():\n","    \"\"\"Test manual labeled test sets.\"\"\"\n","    # outfile = open('data/test.csv', 'w')\n","    # outfile.write(',text,class,predict,question,exclamation,has_num,headline_words\\n')\n","    # outfile.write('text,class,predict')\n","    cnt = 0\n","    labels = []\n","    predicts = []\n","    with open(GOOGLE_DRIVE_PATH+'data/test.csv', 'r') as csvfile:\n","        reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n","        for row in reader:\n","            predict = int(is_good(row['text']))\n","            tmp_str = row['text']+'\",'+str(row['class'])+','+str(predict)+'\\n'\n","            # outfile.write(tmp_str)\n","            cnt += 1\n","            labels.append(int(row['class']))\n","            predicts.append(predict)\n","    # outfile.close()\n","    # print(predicts)\n","    print('F1 Score: %.3f' % f1_score(labels, predicts))\n","    print('Precision Score: %.3f' % precision_score(labels, predicts))\n","    print('Recall Score: %.3f' % recall_score(labels, predicts))\n","out_manual_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mONlcMBJkjPy"},"outputs":[],"source":["def gen_final_test(infilename, outfilename):\n","    csv.field_size_limit(sys.maxsize)\n","    predicts = []\n","    with open(GOOGLE_DRIVE_PATH+f'data/{infilename}', 'r') as csvfile:\n","        for row in csv.DictReader(csvfile, delimiter=',', quotechar='\"'):\n","            title = row['title']\n","            predicts.append(int(is_good(title)))\n","        csvfile.seek(0)\n","        with open(GOOGLE_DRIVE_PATH+f'out/{outfilename}', 'w') as outfile:\n","            writer = csv.writer(outfile)\n","            for idx, row in enumerate(list(csv.reader(csvfile, delimiter=',', quotechar='\"'))):\n","                if idx == 0:\n","                    writer.writerow(row+['stylish_class'])\n","                else:\n","                    writer.writerow(row+[predicts[idx-1]])\n","gen_final_test('100k_updated_with_full_name.csv', '100k_updated_with_full_name_updated.csv')\n","# 100k_updated_with_full_name.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5979,"status":"ok","timestamp":1652191818668 ,"user_tz":240},"id":"ivF5FZV0kjPz","outputId":"fe673980-4f86-4a40-fa72-a6dcb7dd161d"},"outputs":[],"source":["import json\n","import csv\n","def gender_clickbaity():\n","    csv.field_size_limit(sys.maxsize)\n","    abs = []\n","    gender_dict = {'male':0,'female':0,'unk':0,'male_stylish':0,'female_stylish':0, 'unk_stylish':0, 'tot':0, 'tot_stylish':0}\n","    with open(GOOGLE_DRIVE_PATH+'out/100k_updated_with_full_name.csv', 'r') as csvfile:\n","        for row in csv.DictReader(csvfile, delimiter=',', quotechar='\"'):\n","            # first author\n","            gender = json.loads(row['genders'].replace(\"'\", '\"'))\n","            clickbaity = row['stylish_class']\n","            if len(gender) != 0:\n","                gender = gender[0]\n","                # print(gender)\n","            else:\n","                gender_dict['unk'] += 1\n","                gender_dict['unk_stylish'] += int(clickbaity)\n","                continue\n","            if gender == 'M':\n","                gender_dict['male'] += 1\n","                gender_dict['male_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","            elif gender == 'F':\n","                gender_dict['female'] += 1\n","                gender_dict['female_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","            elif gender == '-':\n","                gender_dict['unk'] += 1\n","                gender_dict['unk_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","                if int(clickbaity) == 1:\n","                    abs.append(row['']+'|\"'+row['abstract']+'\"')\n","            else:\n","                raise ValueError(gender)\n","            gender_dict['tot'] += 1\n","        return gender_dict, abs\n","gender_dict, abs = gender_clickbaity()\n","gender_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd6U-5JFS-3X"},"outputs":[],"source":["import json\n","import csv\n","def gender_clickbaity():\n","    csv.field_size_limit(sys.maxsize)\n","    abs = []\n","    gender_dict = {'male':0,'female':0,'unk':0,'male_stylish':0,'female_stylish':0, 'unk_stylish':0, 'tot':0, 'tot_stylish':0}\n","    with open(GOOGLE_DRIVE_PATH+'out/100k_updated_with_full_name.csv', 'r') as csvfile:\n","        for row in csv.DictReader(csvfile, delimiter=',', quotechar='\"'):\n","            # first author\n","            gender = json.loads(row['genders'].replace(\"'\", '\"'))\n","            clickbaity = row['stylish_class']\n","            if len(gender) != 0:\n","                gender = gender[0]\n","                # print(gender)\n","            else:\n","                gender_dict['unk'] += 1\n","                gender_dict['unk_stylish'] += int(clickbaity)\n","                continue\n","            if gender == 'M':\n","                gender_dict['male'] += 1\n","                gender_dict['male_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","            elif gender == 'F':\n","                gender_dict['female'] += 1\n","                gender_dict['female_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","            elif gender == '-':\n","                gender_dict['unk'] += 1\n","                gender_dict['unk_stylish'] += int(clickbaity)\n","                gender_dict['tot_stylish'] += int(clickbaity)\n","                if int(clickbaity) == 1:\n","                    abs.append(row['']+'|\"'+row['abstract']+'\"')\n","            else:\n","                raise ValueError(gender)\n","            gender_dict['tot'] += 1\n","        return gender_dict, abs\n","gender_dict, abs = gender_clickbaity()\n","gender_dict"]},{"cell_type":"markdown","metadata":{"id":"fYrOzCDOMRQV"},"source":["Write the abstracts of paper by authors with unclassified genders to `out/unclassified_abstract.csv`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcHdqNP8K803"},"outputs":[],"source":["with open(GOOGLE_DRIVE_PATH+'out/unclassified_abstract.csv', 'w') as csvfile:\n","        csvfile.write('\\n'.join(abs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3GrQQhRkjPz","outputId":"1756b64c-92b7-4de3-b8c3-1fd0e5564638"},"outputs":[],"source":["print('m_avg: {}'.format(gender_dict['male_stylish']/gender_dict['male']))\n","print('f_avg: {}'.format(gender_dict['female_stylish']/gender_dict['female']))\n","print('avg: {}'.format(gender_dict['tot_stylish']/gender_dict['tot']))"]},{"cell_type":"markdown","metadata":{"id":"2kQYIFflMxjj"},"source":["Detect the language of the unclassified abstracts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lf4ts6pWNddu"},"outputs":[],"source":["!pip install spacy_langdetect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1J7S1gdVOBFy"},"outputs":[],"source":["# spacy detect language of longer sentences\n","from spacy_langdetect import LanguageDetector\n","import spacy\n","nlp = spacy.load('en')\n","nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n","\n","import pandas as pd\n","from collections import defaultdict\n","lang = defaultdict(int)\n","with open(GOOGLE_DRIVE_PATH+f'out/unclassified_abstract.csv', 'r') as csvfile:\n","  for row in csv.reader(csvfile, delimiter='|', quotechar='\"'):\n","    try:\n","      lang[nlp(row[1])._.language['language']] += 1\n","    except IndexError as e:\n","      print(e)\n","      continue"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1652192438649 ,"user_tz":240},"id":"EqjTus01MxEc","outputId":"0f039998-fe03-4c79-8a27-287d55abee62"},"outputs":[],"source":["lang"]},{"cell_type":"markdown","metadata":{"id":"nGWMdY4XMr-M"},"source":["# simply for test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128,"status":"ok","timestamp":1652189121762 ,"user_tz":240},"id":"XW71sThdkjPz","outputId":"d4e683f8-4a0e-42bb-b08f-80fe3cbc5713"},"outputs":[],"source":["import pickle\n","csv.field_size_limit(sys.maxsize)\n","with open(GOOGLE_DRIVE_PATH+f'out/100k_updated_with_full_name.csv', 'r') as csvfile:\n","    cnt = 0\n","    for row in csv.DictReader(csvfile, delimiter=',', quotechar='\"'):\n","      if cnt < 10:\n","        print(row['abstract'])\n","      else:\n","        break\n","      cnt += 1"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"classify.ipynb","provenance":[]},"interpreter":{"hash":"9af3ee2666bbfbf5df1ad6460ba1fe659c07264328d725f4e19fe6b13fee1662"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
